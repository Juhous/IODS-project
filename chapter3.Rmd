# Logistic regression exercise

## Data wrangling
The code used for data wrangling exercise can be found [here](https://github.com/Juhous/IODS-project/blob/master/create_alc.R)

## Load data

```{r}
library(ggplot2)
library(GGally)
library(tidyr)
library(dplyr, warn.conflicts = F)
library(stringr)
source(paste(getwd(),"multiplot.R", sep="/"))

df <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep=",", head = T) %>% tbl_df()

df
summary(df)
```
So, the data explores school performance of portuguese students. This set contains information of 382 students, their grades and several variables that might correlate with performance.

## Hypothesis of high alcohol use predicting variables
I thought that good family relations and extra-curricular activities would protect from high alcohol usage. Age and number of absences would, conversely, increase alcohol usage. 

```{r}
mod <- df %>% select(high_use, famrel, activities, absences, age) 
mod

mod %>% select(famrel:age) %>% sapply(., function(variable_value) table(mod$high_use, variable_value))

p1 <-  ggplot(mod, aes(high_use, famrel)) + geom_boxplot() 
p2 <-  ggplot(mod, aes(high_use, fill = activities)) + geom_bar()
p3 <-  ggplot(mod, aes(high_use, absences)) + geom_boxplot() 
p4 <-  ggplot(mod, aes(high_use, age)) + geom_boxplot() 

multiplot(p1,p2,p3,p4, cols = 2)

```
These values are well in line with the proposed hypothesis, except for the activities. It seems that extra-curricular activities do not protect from high alcohol usage. 

## Logistic regression 
### Model
```{r}
m <- glm(high_use ~ famrel + activities + absences + age, data = df, family = binomial)
summary(m)

```

Based on the logistic regression model shown above, this hypothesis seems to hold true for factors other than activities. 

### Details of model
```{r}
coef(m)
OR <- coef(m) %>% exp()
CI <- confint(m) %>% exp() 
cbind(OR, CI)
```


## Predictions
```{r}
df <- mutate(df, prob = predict(m, type = "response"))
df <- mutate(df, pred= prob > .5)
df
table(high_use = df$high_use, prediction = df$pred)
table(high_use = df$high_use, prediction = df$pred) %>%
  prop.table() %>% `*`(100) %>% round(2) %>% addmargins()
```


## 10-fold CV
```{r}
library(boot)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
loss_func(df$high_use, df$prob)
cv <- cv.glm(data = df, cost = loss_func, glmfit = m, K = 10)
cv$delta[1]
```
So with 10-fold cross-validation, accurasy actually increases from ~28% to ~29%
