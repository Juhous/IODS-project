{
    "collab_server" : "",
    "contents" : "# Logistic regression exercise\n\n## Data wrangling\nThe code used for data wrangling exercise can be found [here](https://github.com/Juhous/IODS-project/blob/master/create_alc.R)\n\n## Load data\n```{r}\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(stringr)\nsource(paste(getwd(),\"multiplot.R\", sep=\"/\"))\n\ndf <- read.table(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt\", sep=\",\", head = T) %>% tbl_df()\n\ndf\nsummary(df)\n```\nSo, the data explores school performance of portuguese students. This set contains information of 382 students, their grades and several variables that might correlate with performance.\n\n## Hypothesis of high alcohol use predicting variables\nI thought that good family relations and extra-curricular activities would protect from high alcohol usage. Number of absences and going more out with friends would, conversely, increase alcohol usage. \n\n```{r}\nmod <- df %>% select(high_use, famrel, activities, absences, goout) \nmod\n\nmod %>% select(famrel:goout) %>% sapply(., function(variable_value) table(mod$high_use, variable_value))\n\np1 <-  ggplot(mod, aes(high_use, famrel)) + geom_boxplot() \np2 <-  ggplot(mod, aes(high_use, fill = activities)) + geom_bar()\np3 <-  ggplot(mod, aes(high_use, absences)) + geom_boxplot() \np4 <-  ggplot(mod, aes(high_use, goout)) + geom_boxplot() \n\nmultiplot(p1,p2,p3,p4, cols = 2)\nrm(p1,p2,p3,p4, mod)\n```\n\nThese values are well in line with the proposed hypothesis, except for the activities; it seems that extra-curricular activities do not protect from high alcohol usage. \n\n## Logistic regression \n### Model\n```{r}\nm <- glm(high_use ~ famrel + activities + absences + goout, data = df, family = binomial)\nsummary(m)\n```\nBased on the logistic regression model shown above, this hypothesis seems to hold true for all proposed factors\n\n### Details of model\n```{r}\ncoef(m)\nOR <- coef(m) %>% exp()\nCI <- confint(m) %>% exp() \ncbind(OR, CI) %>% round(3)\nrm(OR,CI)\n```\nA step towards better family relationship is associated with odds .731, meaning that a risk for high alcohol usage is reduced. Growing number of absences, going out, and a lack of extra-curricular activities tend to lead to more high alcohol usage. \n\n### Predictions\n```{r}\ndf <- mutate(df, prob = predict(m, type = \"response\"))\n\nlibrary(pROC, warn.conflicts = F)\nplot.roc(df$high_use, df$prob)\ndf <- mutate(df, pred = prob > .4)\n\ntable(high_use = df$high_use, prediction = df$pred)\ntable(high_use = df$high_use, prediction = df$pred) %>%\n  prop.table() %>% `*`(100) %>% round(2) %>% addmargins()\n```\nBased on receiver-operating-curve, the threshold for classification should be a little over .3 for optimal accurasy. With a threshold of .4, roughly 77% of predictions are correct. This better than what you should expect from simple toin cossing scenario, so there is some benefit in using our model. \n\n## 10-fold CV\n```{r}\nlibrary(boot)\nloss_func <- function(class, prob) {\n  # adjusted for threshold .4\n  n_wrong <- abs(class - prob - .1) > .5\n  mean(n_wrong)\n}\nloss_func(df$high_use, df$prob)\ncv <- cv.glm(data = df, cost = loss_func, glmfit = m, K = 10)\ncv$delta[1]\nrm(cv, loss_func)\n```\nSo with 10-fold cross-validation, accurasy decreaes from ~77% to ~75%\n  ",
    "created" : 1511386989948.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1586284856",
    "id" : "D06AA540",
    "lastKnownWriteTime" : 1511397313,
    "last_content_update" : 1511397325419,
    "path" : "~/Desktop/IODS/IODS-project/chapter3.Rmd",
    "project_path" : "chapter3.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}