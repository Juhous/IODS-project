{
    "collab_server" : "",
    "contents" : "# Linear regression exercise\n\n## Data wrangling\nScript for data wrangling is [here](https://github.com/Juhous/IODS-project/blob/master/create_learning2014.R)\n\n## Load the data\n\n```{r}\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(stringr)\n\nanalysis <- read.csv(\"data/learning2014.csv\") %>% tbl_df()\nglimpse(analysis)\n```\n\nThe data contains learning results (points) of 166 students and the possibly associated variables: attitude, age, gender. Also icluded are students' likert-scores on following dimensions of learning : deep, surface/superficial and strategic. \n\n## Complete description of study variables\nVariables starting with ST, SU, and Dnum present dimensions strategic, superficial, and deep respectively.\n\n```{R}\nlibrary(knitr)\nlibrary(kableExtra)\ninfo <- read.delim(\"http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt\", \n           header = F, sep = \"\\t\", encoding = \"latin1\")[12:71,] %>% \n  str_split(boundary(type = \"word\"), n = 2, simplify = T) %>% as.data.frame()\nnames(info) <- c(\"Variable\", \"Description\") \nkable(info, \"html\") %>% \n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\")) %>%\n  scroll_box(height = \"200px\")\n```\n\n## Exploratory analysis of study variables\n```{R, message = F, out.width = \"100%\"}\nggpairs(analysis,       \n  upper = list(continuous = wrap(\"cor\", size = 3)), \n  lower = list(continuous = wrap(\"points\", alpha = .2, size = .6),\n               combo = wrap(\"facethist\", bins = 10))) +\n  theme(\n    axis.text.x = element_text(angle = 90, color = \"black\", size = 7, vjust = .5),\n    axis.text.y = element_text(color = \"black\", size = 7))\n\nsummary(analysis)\n```\n\nAll variables seem to be normally distributed. There is quite a bit more female subjects than males  (`r sum(analysis$gender==\"F\")` vs `r sum(analysis$gender==\"M\")`). By eyeballing histograms above, there seems to be no great differences between genders, except maybe for the attitude-variable. Points, our dependent variable, has a bit of a notch on the left tail, but still reasonably follows normality. For variable age, logarithmic conversion might help to increase normality. \n\n## Generating linear model for predicting points\n### Model #1\nI chose to use attitude, deep and stra as explanatory variables for points, as they were most strongly correlated with it according to the exploratory analysis. \n\n```{R, out.width = \"100%\"}\nmodel <- lm(Points ~ Attitude + deep + stra, data = analysis)\nsummary(model)\n```\n\nIn this model only attitude significantly predicts points, so other 2 variables are excluded. Low p-value for deep-learning probably arises from the fact that Attitude and deep learning are highly correlated (r=0.8)\n\n### Model #2\n```{R, out.width = \"100%\"}\nmodel <- lm(Points ~ Attitude, data = analysis)\nsummary(model)\n```\n\nSo the attitude of the student seems to correlate with how much points (s)he will score in the test; better attitude predicts better scores. R-squared value explains how big proportion of variance of the dependent variable (points) is explained by the model. In this case only 18.5% of the variability in points is explained by the proposed model.\n\n### Diagnostic plots\n\n```{r, out.width=\"100%\"}\npar(mfrow = c(2,2), oma = c(0, 0, 2, 0), mar = c(2.5,3,2,0.5), mgp = c(1.5,.5,0))\nplot(model, which = c(1,2), add.smooth = T)\n\nnorm.res <- model$residuals/(sqrt(deviance(model)/df.residual(model))*sqrt(1-hatvalues(model)))\n# Counted the normalized residuals long way for fun. Following code can be used to check results\n# sum(norm.res != rstandard(model))\n\naa <- analysis$Attitude\nleverage <- (aa-mean(aa))^2/sum((aa-mean(aa))^2)+1/length(aa)\n\nplot(leverage, norm.res, xlab = \"Leverage\", ylab = \"Standardized residuals\")\nplot(cooks.distance(model), norm.res, xlab = \"Cook's distance\", ylab = \"Standardized residuals\")\n\n\n```\n\n### Conclusions\nAccording to diagnostic plots, this model has no critical errors:  \n1. Residuals are the difference between fitted and the actual value. In this plot we see no clustering, or any other patterns, that could indicate problems in the model. **Variance of errors is constant**  \n2. On Q-Q plot, on the extreme values, the model loses some of its accuracy. In other words, the model overestimates performance of students with either very positive or negative attitude. Shapiro-Wilkes test doesn't agree with normality, however: \"shapiro.test(rstandard(model))\\$p.value)\" which gives p = `r shapiro.test(rstandard(model))$p.value`. **Erros in this model are distributed normally *enough.* **  \n3. Leverage describes the unusualness of predictor values. For any individual variable, it tells how extreme, eg. how far of variable mean any particular observation is. For multivariate model, these \"hatvalues\" take into consideration \"combined unusualness\" across variables -- observation might not be far from mean in any single variable, but combination of those values might be. **In this model, no observations have high leverage.**  \n4. Cook's distance tells how much fitted values would change if the observation in question is deleted -- it can be used to diagnose and remove influential outliers. **No single observation affects model too much.**  \n<br>",
    "created" : 1510149838539.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1205494546",
    "id" : "83814427",
    "lastKnownWriteTime" : 1511034674,
    "last_content_update" : 1511034674,
    "path" : "~/Desktop/IODS/IODS-project/chapter2.Rmd",
    "project_path" : "chapter2.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "content_preview_enabled" : "true",
        "content_preview_inline" : "false",
        "docOutlineVisible" : "1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}