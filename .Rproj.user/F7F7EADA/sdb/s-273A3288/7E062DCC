{
    "collab_server" : "",
    "contents" : "# Clustering and classification\n\n## 2 Loading data\n```{r, message = FALSE}\n#Hidden\nsource(\"helper_functions.R\")\nlibrary(magrittr)\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(MASS)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(corrplot)\nlibrary(plotly)\nlibrary(dplyr)\n```\n```{R}\ndata(\"Boston\")\ndf <- Boston %>% tbl_df() %>% \n  mutate_at(vars(chas), funs(as.factor))\nglimpse(df)\n```\n```{R, echo = FALSE}\nhelp(Boston)\nrm(Boston)\n```\n14 variables, for which complete description can be found [here](\"https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html\"). In short, the dataset has information of housing values in different suburbs of Boston. Study includes multiple variables describing safety, housing density, and accessibility, for example.\n\n\n## 3 Exploring data\n### Start\n\n```{R, out.width = \"100%\"}\n#Hidden\nsummaryKable(df) %>% \n  kable(\"html\", align = \"rrr\", caption = \"Data variable summary\") %>%\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\")) %>% \n  scroll_box(width = \"100%\")\n```\n\n```{R, out.width = \"100%\"}\n#Hidden\nggpairs(df,\n  title = \"Study variable overview\",\n  upper = list(continuous = wrap(\"cor\", size = 2)),\n  lower = list(\n    continuous = wrap(\"points\", alpha = .2, size = .3),\n    combo = wrap(\"facethist\", bins = 20))) +\ntheme(axis.text.x = element_text(\n                  angle = 90,\n                  color = \"black\",\n                  size = 6,\n                  vjust = .5),\n      axis.text.y = element_text(color = \"black\", size = 6))\n```\nThe data is quite spread out. \n\n## 4 Scaling and sampling\n### Scaling\n```{R}\n# Scale function adds attributes, and ggpairs doesn't like it\ndf %<>% mutate_at(vars(-chas), funs(scale)) %>%\n  mutate_at(vars(-chas), funs(as.vector)) \n\ndf$crim <- ntile(df$crim, 4) %>% \n  factor(labels = c(\"low\", \"med-lo\", \"med-hi\", \"high\"))\ntable(df$crim)\n```\n\n```{r, out.width = \"100%\"}\n#Hidden\nsummaryKable(df) %>% \n  kable(\"html\", align = \"rrr\", caption = \"Data variable summary\") %>%\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\")) %>% \n  scroll_box(width = \"100%\")\n```\n\n```{r, out.width = \"100%\"}\n#Hidden\nggpairs(df,\n  title = \"Study variable overview\",\n  upper = list(continuous = wrap(\"cor\", size = 2)),\n  lower = list(\n    continuous = wrap(\"points\", alpha = .2, size = .3),\n    combo = wrap(\"facethist\", bins = 20))) +\ntheme(axis.text.x = element_text(\n                  angle = 90,\n                  color = \"black\",\n                  size = 6,\n                  vjust = .5),\n      axis.text.y = element_text(color = \"black\", size = 6))\n```\nasldkfj\n\n### Sampling to test and train sets\n```{r}\nsample_ind <- sample(nrow(df),  size = nrow(df) * 0.8)\ntrain <- df[sample_ind,]\ntest <-  df[-sample_ind,]\ndata.frame(dim(train),dim(test))\n```\nasdf\n\n## 5 Linear discriminant analysis\n### Model and plot\n```{R}\nlda.fit <- lda(crim ~ ., data = train)\nlda.fit\n```\n\nI made it 3D for fun \n```{r, out.width = \"100%\"}\n#Hidden\n# plot the lda results\npoints <- data.frame(crim = train$crim,\n                     lda = predict(lda.fit)$x)\nlevels(points$crim) %<>%  str_to_title()\n\narrows <- coef(lda.fit) %>% \n  data.frame(., label = rownames(.)) %>% arrange(desc(abs(LD1))) %>% \n  mutate(LD1 = LD1*2.5, LD2 = LD2*2.5, LD3 = LD3*2.5, pos = 1) %>% \n  rbind(., mutate(., LD1=0, LD2=0, LD3=0, pos =0)) \n\n\np1 <- plot_ly(arrows, x = ~LD1, y = ~LD2, z = ~LD3, \n  type = \"scatter3d\" , color = ~label, colors = rep(rgb(0, 0, 0), 13),\n  opacity = .5, mode = \"lines\", hoverinfo = \"name\", showlegend = FALSE, \n  line = list(width = 5))\n\np2 <- plot_ly(points, x = ~lda.LD1, y = ~lda.LD2, z = ~lda.LD3, \n    type = \"scatter3d\" , color = ~crim, opacity = .5, hoverinfo = \"none\",\n    mode = \"markers\", marker = list(size = 3, width = 2)) %>% \n  layout(title = \"PCA\",\n       scene = list(xaxis = list(title = \"LDA1\"),\n                    yaxis = list(title = \"LDA2\"),\n                    zaxis = list(title = \"LDA3\")))\n\nsubplot(p1, p2)\n```\n\n\n## 6 Testing the model. \n```{R}\ntable(\"Crime\" = test$crim, \n      \"Prediction\" = predict(lda.fit, newdata = test)$class)\n```\nMany comments here\n\n## 7 Kmeans\n### Finding optimal number\n```{r}\ndata(\"Boston\")\ndf <- Boston %>% tbl_df\ndf %<>% mutate_at(vars(-chas), funs(scale)) %>% \n  mutate_at(vars(-chas), funs(as.vector)) \n  \ncbind(Bost = summary(dist(Boston)), df = summary(dist(df))) %>% t()\n\nkm <- kmeans(df, centers = 3)\n\ntwcss <- sapply(1:20, function(k) kmeans(df, k)$tot.withinss)\nqplot(x = 1:20, y = twcss, geom = 'line')\n```\n\nBased on this, optimal value around 3 centers.\n\n### Km with optimal number of clusters\n```{R}\nkm <- kmeans(df, centers = 3)\n```\n```{r, out.width = \"100%\"}\n#Hidden\ndf %>% mutate(clust = factor(km$cluster)) %>%\n  gather(., key = \"var\", value = \"value\", -c(crim, clust)) %>%\n  ggplot(aes(value, crim, color = clust)) + \n  geom_point(shape = 1, size = 1.5, stroke = 1, alpha = .3) + \n  facet_wrap(~var, scales = \"free_x\") \n```\nTax, \n\n## Bonus\n```{r}\n# Add clusters of 3-centered kmeans to normalized boston dataset and remove variable crim\ndf %<>% mutate(clust = factor(km$cluster)) %>% select(-crim)\nlda.clust <- lda(clust ~ ., data = df)\nlda.clust\n```\n```{r}\n#Hidden\npoints <- data.frame(clust = df$clust,\n                     lda = predict(lda.clust)$x)\nnames(points) <- c(\"Cluster\", \"LD1\", \"LD2\")\n\narrows <- coef(lda.clust) %>% \n  data.frame(., label = str_to_upper(rownames(.))) %>% \n  arrange(desc(abs(LD1))) %>% \n  # Scale the arrows \n  mutate(LD1 = LD1*5, LD2 = LD2*5, pos = 1) \n\nggplot(points) +\n  theme_minimal() +\n  geom_point(aes(LD1, LD2, color = Cluster), \n             shape = 1, size = 2, stroke = 1.5, alpha = .75) +\n  geom_segment(data = arrows, \n               aes(y=0, x=0, yend=LD2, xend=LD1, alpha = .5), \n               arrow = arrow(length = unit(0.075, \"inches\")),\n               show.legend = F) +\n  # Adjust the labels .2 units away from the arrowhead\n  geom_text(data = arrows, aes(x = LD1+.2*(LD1/sqrt(LD1^2+LD2^2)), \n                               y = LD2+.2*(LD2/sqrt(LD1^2+LD2^2)), \n                               hjust = .5,\n                               label = label), \n            show.legend = F) \n```\nThe strongest predictors of clusters are tax, nox, zn, and age. \n\n## Superbonus\n```{R}\nmodel_predictors <- dplyr::select(train, -crim) %>% \n  mutate(chas = as.numeric(chas))\n\n# check the dimensions\ndata.frame(dim(model_predictors), dim(lda.fit$scaling))\n\n# matrix multiplication\nmatrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling\nmatrix_product <- as.data.frame(matrix_product) \nhead(matrix_product)\n```\n```{r, out.width = \"100%\"}\n#Hidden\np1 <- plot_ly(data = matrix_product, x = ~LD1, y = ~LD2, z = ~LD3, \n        type= 'scatter3d', mode='markers', color = train$crim, \n        marker = list(size = 3, width = 2), scene = \"scene1\") \n\n# I run the Kmeans modeling with all observations, so I need to either adjust by selecting only relevant observations \ntrain_km <- as.data.frame(km$cluster)[sample_ind,] %>% as.factor()\n\np2 <- plot_ly(data = matrix_product, x = ~LD1, y = ~LD2, z = ~LD3, \n        type= 'scatter3d', mode='markers', color = train_km,\n        marker = list(size = 3, width = 2), scene = \"scene2\")\n\nsubplot(p1,p2) %>% \n  layout(scene1 = list(domain=list(x=c(0,0.5),y=c(0,1)), \n                       xaxis = list(title = \"LD1\"), \n                       yaxis = list(title = \"LD2\"),\n                       zaxis = list(title = \"LD3\")),\n         scene2 = list(domain=list(x=c(0.5,1),y=c(0,1)),\n                       xaxis = list(title = \"LD1\"), \n                       yaxis = list(title = \"LD2\"),\n                       zaxis = list(title = \"LD3\")))\n```\n\nMost obvious difference is of course that the first lda-plot has 4 classes whereas the km-plot only has 3. Clustering is done fairly similarly, both models easily distinguish the group with very positive LD1 values. Kmeans seems to achieve (at least visually judging) better separation in the other, larger, cluster.\n",
    "created" : 1512577915186.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2614735049",
    "id" : "7E062DCC",
    "lastKnownWriteTime" : 1512581097,
    "last_content_update" : 1512581097843,
    "path" : "~/Desktop/IODS/IODS-project/chapter4.Rmd",
    "project_path" : "chapter4.Rmd",
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}