# Dimensionality reduction exercise
## The data
### 1. Load and describe data
Code for data wrangling can be found [here](https://github.com/Juhous/IODS-project/blob/master/create_human.R)

```{r, message = FALSE}
#Hidden
source("helper_functions.R")
library(magrittr)
library(GGally)
library(stringr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(tidyr)
library(MASS)
library(dplyr)
```
```{r}
df <- read.csv(file = "data/human.csv", row.names = 1) 
glimpse(df)
```
Dataset contains information of human development charasteritics. It totals 189 observations (countries) and 8 variables. 

### 2. Graphical overview of data
```{r, out.width = "100%"}
#Hidden
df %>% summaryKable() %>% 
  kable("html", align = "rrr", caption = "Data variable summary") %>%
  kable_styling(bootstrap_options = c("hover", "condensed")) %>% 
  scroll_box(width = "100%")
```
```{R, warning = F, out.width = "100%"}
# Hidden
ggpairs(df,
  title = "Study variable overview",
  upper = list(continuous = wrap("cor", size = 3)),
  lower = list(
    continuous = wrap("points", alpha = .2, size = .4),
    combo = wrap("facethist", bins = 20))) +
theme(axis.text.x = element_text(
                  angle = 90,
                  color = "black",
                  size = 6,
                  vjust = .5),
      axis.text.y = element_text(color = "black", size = 6))
```
Many of the variables are normally distributed, but with eduRatio, GNIC and matMort, some problems might arise if normality is assumed. 
```{r}
#Hidden
cor(df, use = "pairwise.complete.obs") %>% 
  corrplot::corrplot(method = "number", type = "upper", 
                     tl.pos = "td", tl.srt = 25, 
                     tl.offset = 1, tl.cex = .7,
                     number.cex = .75, 
                     title = "Correlation between study variables",
                     mar=c(0,0,1,0),
                     diag = F) 
```

According to above plot, education correlates strongly and positively with life expectancy, and negatively with maternal mortality and adolescent births. Maternal mortality correlates strongly and negatively with education ratio, life expectancy and education. 

## Principal component analysis
### 3. PCA on non-normalized data
```{r}
pca <- prcomp(df)
pca %>% summary()
```
```{r, warning=F}
#Hidden
biplot(pca, choices = 1:2, cex = c(.5,1))
```

Based on this plot, it seems tahat GNI per capita and maternal mortality are the most essential explanatory components of this data set. 

### 4. PCA on normalized data
```{r}
df_scaled <- scale(df)
pca_scaled <- prcomp(df_scaled)
pca_scaled %>% summary
```
```{r, warning=F}
#Hidden
biplot(pca_scaled, choices = 1:2, cex = c(.5,1))
```

According to scaled data, there is much more variation to what variables explain most of the variance in the data. Here, PC1 explains 50% of variation and most important determinary variables are maternal mortality, life expectancy and so forth. PC2 only explains 16%, and the most significant variables are labour ratio and parliamentary representation. 

### 5. Interpret differences
```{r}
rbind(summary(pca)$importance[2,], summary(pca_scaled)$importance[2,]) %>%
  as.data.frame(row.names = c("orig", "scaled"))
```

The total explained variance decreases with the scaled data. Still, only 3 principal components explain 77% of the variance in the data, and with 5 >90% of variance is explained.

```{r}
orig <- abs(pca$rotation) %*% summary(pca)$importance[2,]
scaled <- abs(pca_scaled$rotation) %*% summary(pca_scaled)$importance[2,] 
cbind(orig, scaled) %>% data.frame() %>% 
  rename(orig = X1, scaled = X2) %>% round(3) %>% t 
```

We can study the how much of the PCA-variance is explained by a variable as shown above. So, we calculate the importance_of_PC*absolute_value_for_variable, and sum this value across PCs for any given variable. From the results, it becomes clear that maternal mortality and GNI per capita were crudely overrepresented in the non-scaled data, most likely because of their high absolute range. With scaled data, the amount of explained variation is distributed fairly equally within variables.

## 6. MCA with tea dataset
```{r}
library(FactoMineR)
data(tea)
df <- tea

mca <- df %>% select(breakfast, friends, resto, Tea, Sport) %>%
  MCA(graph = FALSE)
mca %>% summary
```
```{r}
#Hidden
mca %>% plot(invisible=c("ind"))
```

Based on the plot, the 2 main components describe ~40% of variation in data. Most influential variable seems to be green.

